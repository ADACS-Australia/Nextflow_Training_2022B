---
title: "Astronomy Workflow: Idea to Implementation"
teaching: 30
exercises: 30
questions:
- How do I develop a workflow from scratch?
objectives:
- Create a workflow from scratch and see ut run.
keypoints:
- 
---


## The idea
We wish to create a workflow which will process observing data from some telescope, find candidate pulsars, and then run a ML on these candidates to label them as confirmed or rejected.
Humans will then sort through the confirmed list to come up with a final decision on which are real and which are not.

The high level workflow therefore looks like this:

![Initial workflow]({{page.root}}{% link fig/AstroWFInitial.png%})

As we think more about how the processing will be done we come up with some intermediate steps for the tasks:

![Final workflow]({{page.root}}{% link fig/AstroWFFinal.png %})

In this final iteration we have indicated with arrows the dependencies of each task, but also where information needs to be passed.
For example the "fold data" task needs the multi-frequency data from the "compile" task, as well as the candidate pulsar details from the "find" task in order to do a more detailed measurement of the properties.

## The process

### Step 1 initial channel
Our data is stored as a set of files in the `data/observations` directory with names like `obs_1.dat`.
The meta-data for each file is stored in the file header.
In our example we don't have any actual data in the files, just meta-data, and the files look like this:

> ## obs_1.dat
> ~~~
> #freq: 10 MHz
> #date: 2022-11-15
> #point: 10deg_10deg
> ~~~
> {: .language-bash}
{: .callout}

Our initial channel will be a list of all the files in the `data/observations` directory that look like `obs_*.dat`.
We will leave the data directory as a user parameter with a default value of `observations`.
We create our channel as follows:
~~~
params.data_dir = "observations"

all_obs = Channel.fromPath("${params.data_dir}/*.dat")
~~~
{: .language-groovy}

## Extracting meta-data
The first step in our workflow is to extract the metadata from the files.
For this we define a process called `get_meta`.
Since this will be the first process in the workflow we set it up to accept input as single files, and it will return a tuple of the frequency and pointing direction for the given file, as well as the file itself.

~~~
process get_meta {
    // Convert an input file into a stream with meta data
    // output will be [frequency, point, file]
    input:
    file(obs)

    output:
    tuple (env(FREQ), env(POINT), file(obs))

    script:
    """
    FREQ=\$(grep -e "#freq:"  ${obs} | awk '{print \$2}')
    POINT=\$(grep -e "#point:"  ${obs} | awk '{print \$2}')
    """
}
~~~
{: .language-groovy}

Things to note above:
- We can return variables from our (bash) scripts by saving them as environment variables, and telling NextFlow that the output type is env.
- We have escaped bash `$` with a `\`.
- We don't modify or create any files here, we just extract variables, so the script will take almost no time to run.

To test how we are going so far we can create a stub of a workflow in the same file with some `.view()` commands on the various channels that we are using.

~~~
workflow {
    all_obs.view() // view channel
    get_meta(all_obs) // run process
    get_meta.out.view() // view the channel generated by the output
}
~~~
{: .language-groovy}

We run the above with `nextflow run astro_wf.nf` and it gives us the following:

~~~
$ nextflow run astro_wf.nf 
N E X T F L O W  ~  version 22.10.1
Launching `astro_wf.nf` [zen_einstein] DSL2 - revision: 5338dcdf52
executor >  local (9)
[17/bcae92] process > get_meta (9) [100%] 9 of 9 âœ”
data/observations/obs_4.dat
data/observations/obs_2.dat
data/observations/obs_9.dat
data/observations/obs_3.dat
data/observations/obs_7.dat
data/observations/obs_8.dat
data/observations/obs_1.dat
data/observations/obs_6.dat
data/observations/obs_5.dat
[30, 20deg_10deg, data/work/3d/e3663a45357b9818a547c2908701da/obs_6.dat]
[10, 20deg_10deg, data/work/24/69ad9546cacb3ff01b6199977ab80d/obs_4.dat]
[30, 30deg_10deg, data/work/e5/44d3d5698dc36454861601e4665a32/obs_9.dat]
[20, 30deg_10deg, data/work/bd/e29f1747bd01c8cc8f2278f29f1b17/obs_8.dat]
[20, 10deg_10deg, data/work/33/a37ad4d85131c77467126e0d361667/obs_2.dat]
[30, 10deg_10deg, data/work/0f/22afa02fd765ca828341a6aa91630d/obs_3.dat]
[10, 10deg_10deg, data/work/f4/c5618dd7b5aa66b23c3ce4c1ddc1d3/obs_1.dat]
[10, 30deg_10deg, data/work/19/8411e3effcb8eb6b13d1ae450b3256/obs_7.dat]
[20, 20deg_10deg, data/work/17/bcae92db0ed291c969217d979374a8/obs_5.dat]
~~~
{: .output}

We can see from the above that the input files are `obs_?.dat`, and that the `get_meta` process then turns these into the required tuple of `[freq, pointing, file]`.
Note that when the files are passed to `get_meta` they are symlinked to a new work folder so the file names remain the same but the directory path is updated.

At this point we can see that the `get_meta` is working as intended so we can move on.

## compiling multi-frequency data


### Create a .config file
This will create a bunch of useful analysis for your pipeline run when it completes.
See [next lesson]({{page.root}}{% link _episodes/05-Nextflow_Orchestration.md %}) for more about configuration files.

> ## nextflow.config
> ~~~
> // turn on all the juicy logging
> trace.enabled = true
> timeline.enabled = true
> report.enabled = true
> dag {
>     enabled = true
>     file='dag.png'
> }
> ~~~
> {: .language-groovy}
{: .callout}